# prueba_medica.py
import time
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# 1. ConfiguraciÃ³n Visual (Vibe Coding)
print("ðŸ”µ Inicializando Sistema de Tesis Hospitalario...")
start_time = time.time()

# 2. ConexiÃ³n con el Modelo Local
# Usamos el modelo 1B que descargamos para mÃ¡xima velocidad en CPU
llm = OllamaLLM(model="llama3.2:1b")

# 3. Definir la personalidad del Asistente
prompt = ChatPromptTemplate.from_template("""
Eres un asistente virtual empÃ¡tico y eficiente del Hospital Clinico Magallanes.
Tu objetivo es informar a los familiares sobre el estado de los pacientes de forma clara y tranquila.

Consulta del usuario: {question}

Respuesta breve y profesional:
""")

# 4. Crear la cadena de pensamiento (Chain)
chain = prompt | llm

# 5. Simular una consulta
pregunta = "Â¿En que aÃ±o fue fundado en magallanes, el Hospital Clinico Magallanes?"
print(f"ðŸ‘¤ Usuario: {pregunta}")
print("... Pensando (Ejecutando en CPU local) ...")

respuesta = chain.invoke({"question": pregunta})

# 6. Mostrar resultado
end_time = time.time()
print(f"\nðŸ¤– Asistente IA:\n{respuesta}")
print(f"\nâš¡ Tiempo de respuesta: {end_time - start_time:.2f} segundos")